\section*{ABSTRACT}
Autonomous mobile robots are vital in areas such as logistics and healthcare, where they rely on complex navigation systems to effectively control in dynamic indoor environments. Conventional 2D LiDAR systems, while favourable for obstacle detection in a single plane. However, they perform poorly in recognising low-lying and complex obstacles, limiting their operational efficiency and safety.
This study presents a novel sensor fusion method that combines the 2D wide range of LiDAR with the 3D vision data from depth camera. By converting depth image into LiDAR messages, the robot-accessible environmental data is enriched, resulting in significantly enhanced obstacle detection in multiple dimensions. This fusion technique significantly closes the gap in 2D LiDAR obstacle detection capabilities.
This sensor fusion algorithm is implemented based on enhancements to existing algorithms in the ROS2 framework, simulated and evaluated in Gazebo and RViz2. The enhanced algorithm was combined with the fusion algorithm of this study to consider and improve the problems that can arise when multiple fusions are performed.
\vspace{0.5cm}

\textbf{Keywords:} Mobile Robot, Sensor Fusion, LiDAR, Depth Camera, Obstacle Detection, Indoor Navigation, Mapping

\vspace{0.5cm}
% \textbf{NOMENCLATURE}
% \\
% $\alpha \quad$ alpha
% \\
% $\beta \quad$ beta
% \\
% $\varphi \quad$ phi