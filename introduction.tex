\section{INTRODUCTION}
\label{sec:introduction}

The emergence and popularity of indoor mobile robots has dramatically altered various fields 
such as logistics, healthcare, and home services, 
allowing for the automation of navigation tasks in indoor environments. 
Among them, Autonomous mobile robots are used in a variety of fields 
to improve services and daily activities, including households, hospitals, businesses, 
institutions, agriculture, and industry~\cite{reviewmobile}.

Indoor autonomous robots rely heavily on advanced navigation systems 
that can safely navigate complex and changing indoor environments. 
One important feature of such systems is their ability to detect and avoid obstacles, 
which is critical for operational efficiency and safety. 
These robots navigate autonomously, determining their position in the environment 
and plotting a collision-free path to their destination.
It must be essential for these robots to gather information from their surroundings 
and perceive objects or relative positions around them. 
Perception is an important aspect of mobile robotics research. 
Tasks like accurately estimating the position of an object may become difficult 
if the mobile robot is unable to observe its surroundings correctly and efficiently~\cite{reviewmobile}. 
This process necessitates complex interactions among sensor inputs, data processing, 
and actuation mechanisms~\cite{perception}.

Among the sensors used for this purpose, 
LiDAR stands out for its ability to accurately measure distance and detect obstacles. 
LiDAR is an acronym for ``light detection and ranging'', also known as ``Laser Scanning Radar''. 
Despite these benefits, LiDAR has inherent limitations in its detection capabilities. 
These limitations include, in particular, 
the high cost of 3D LiDAR and its massive computational requirements, 
as well as 2D LiDAR's detection dimension limitations.

The 2D LiDAR system is often installed on top of the robot, 
allowing it to detect obstacles in the horizontal plane of its laser beams. 
However, it is difficult to detect obstacles that do not intersect this plane, 
particularly those near the ground (e.g., chair supports or lamp bases). 
This detection capability gap highlights the need for 2D LiDAR navigation systems to recognise obstacles in multiple dimensions. 
To address the challenge of detecting low-lying obstacles, 
this project presents a LiDAR and camera system by converting depth camera data into a LiDAR message (LaserScan Message). 
This method combines the 3D visual information provided by the camera with the 360-degree measurements provided by the LiDAR, 
resulting in a synergistic effect and increased environmental awareness. 
The camera captures detailed images of the environment, 
including objects below the LiDAR detection plane, 
whereas the LiDAR aids spatial understanding by accurately mapping obstacles at greater distances and over a larger area.
ROS2 includes the software package ``Depthimage\_to\_Laserscan''~\cite{rosdepthimage}. 
However, the algorithm is configured to use the depth camera as a single sensor for obstacle avoidance information.

As a result, adapting the principal from this package 
to convert depth information from the camera into LiDAR information 
and then integrating it with LiDAR data via a series of algorithms, 
a comprehensive environment model can be created. 
This method significantly improves the robot's ability to detect and avoid direct 
and low-lying obstacles, increasing the safety and efficiency of indoor navigation.
The LiDAR-camera fusion approach resolves a critical sensor selection conflict, 
closes the detection capability gap in 2D LiDAR systems. 
Also, it improves the autonomy and efficiency of indoor robots, 
advancing the field of autonomous robot navigation.